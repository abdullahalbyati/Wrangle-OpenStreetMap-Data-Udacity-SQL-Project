{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenStreet Map San Francisco California Financial District Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I work in this area and wanted to do the study on this area to find what the OpenStreet data shows. \n",
    "The data was exported using the OpenStreetMap export tool (Linked below) by zooming in to the area of intrest and clicking on the Overpass API link\n",
    "\n",
    "[San Francisco California Financial District](https://www.openstreetmap.org/export#map=15/37.7829/-122.4117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the map XML file and parrsing it using xml library in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries for this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import schema\n",
    "import sqlite3 \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the iterative parsing to process the map file and\n",
    "find what tags are there, and how many of each of those tags in the file, to get the\n",
    "feeling on how much of which data you can expect to have in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'note': 1,\n",
       " 'meta': 1,\n",
       " 'bounds': 1,\n",
       " 'tag': 143144,\n",
       " 'node': 235055,\n",
       " 'nd': 292789,\n",
       " 'way': 26374,\n",
       " 'member': 81648,\n",
       " 'relation': 1500,\n",
       " 'osm': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a function called count_tags which iterate through the xml file looking for tags\n",
    "and count those tags.\n",
    "\"\"\"\n",
    "def count_tags(file):\n",
    "    tags = {} #create empty dic to hold values of tags and their counts\n",
    "    for ev,elem in ET.iterparse(file):\n",
    "        tag = elem.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag]+=1\n",
    "    return tags\n",
    "count_tags('SF_MAP.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the \"k\" value for each tag and see if there are any potential problems.\n",
    "count of each of\n",
    "\n",
    "four tag categories in a dictionary:\n",
    "\n",
    "- \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "- \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "- \"problemchars\", for tags with problematic characters, and\n",
    "- \"other\", for other tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 104477, 'lower_colon': 37526, 'problemchars': 36, 'other': 1105}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "            if lower.search(element.attrib['k']) != None: \n",
    "                keys['lower'] += 1\n",
    "            elif lower_colon.search(element.attrib['k']) != None:\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(element.attrib['k']) != None:\n",
    "                keys['problemchars'] += 1\n",
    "            else: \n",
    "                keys['other'] += 1\n",
    "                \n",
    "        \n",
    "    return keys\n",
    "\n",
    "def process_map(file):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(file):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "process_map('SF_MAP.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the number of users that contributed to this area of the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Contributors To This Map Area is: 915\n"
     ]
    }
   ],
   "source": [
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "def process_map_user(file):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(file):\n",
    "        if element.tag == 'node' or element.tag == 'way' or element.tag == 'relation':\n",
    "            users.add(element.attrib['user'])\n",
    "    print(\"Total Contributors To This Map Area is:\", len(users))\n",
    "process_map_user('SF_MAP.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Auditing (Encoutered Problems)\n",
    "First I will create an auditing function called audit_address which looks through the provided XML file and the content we are trying to audit and return the valuse that need auditing. Content can be street and postal code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_address(filename, content):\n",
    "        key='addr:'+ content\n",
    "        expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\",\"Way\", \"Circle\", \"Key\",\"Terrace\", \"Garden\"]\n",
    "    \n",
    "# When the content is 'postcode', the function audits the validity of the postcode\n",
    "# For our map we need zip codes that start with 94 \n",
    "# Also for consistency only use 5 digits zip codes\n",
    "\n",
    "        if content=='postcode':\n",
    "            i=0   #set a records tracker \n",
    "            for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "                i+=1\n",
    "                if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "                    for tag in elem.iter(\"tag\"):\n",
    "                        if tag.attrib['k']== key:\n",
    "                            if (tag.attrib['v'][0:2]!='94') or (len(tag.attrib['v'])!=5): \n",
    "                                print(i)\n",
    "                                print (tag.attrib['v'])\n",
    "\n",
    "# When the content is 'street', the function audits the validity of the street name\n",
    "# It returns a dictionary with street types as keys and corresponding counts as values. \n",
    "\n",
    "        elif content=='street':\n",
    "            \n",
    "            street_types = {}\n",
    "            street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "            for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "                if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "                    for tag in elem.iter(\"tag\"):\n",
    "                        if tag.attrib['k']== key:\n",
    "                            m = street_type_re.search(tag.attrib['v'])\n",
    "                            if m:\n",
    "                                street_type = m.group() #group(): Return the string matched by the RE\n",
    "                                if street_type not in expected:\n",
    "                                    street_types[street_type]=street_types.get(street_type,0)+1\n",
    "            print (street_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings From Auditing**\n",
    "\n",
    "let's run the audit_address we created in the code snippet above for street names and postal codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5855\n",
      "94103-3124\n",
      "232094\n",
      "90214\n",
      "232099\n",
      "90214\n",
      "232104\n",
      "90214\n",
      "237763\n",
      "95115\n",
      "246389\n",
      "94115-4620\n",
      "266269\n",
      "914105\n",
      "610582\n",
      "90214\n",
      "610606\n",
      "90214\n",
      "610716\n",
      "90214\n"
     ]
    }
   ],
   "source": [
    "# Zip codes that don't start with 94 or has more than 5 numbers\n",
    "audit_address(\"SF_MAP.xml\", \"postcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above there are 20 zip codes that don't meet the specification of 5 digits or starts with 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Park': 7, 'Plaza': 8, 'Embarcadero': 17, 'St': 9, 'Center': 5, '730': 2, 'B': 1, 'A': 1, 'Building': 2, '3658': 1, 'Rock': 1, 'st': 2, 'Broadway': 41, 'Montgomery': 1, 'Hyde': 1, 'Post': 1, 'Kearny': 1, 'Pier': 1, 'Bldg': 1, 'St.': 3, 'Fell': 1, 'Ctr.': 1, '4.5': 1, 'North': 3, 'Alley': 5, 'Ave': 1, 'California': 1}\n"
     ]
    }
   ],
   "source": [
    "# Street Names\n",
    "audit_address(\"SF_MAP.xml\", \"street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the street auditing we can see the following that will need cleaninf before we move the data to a database:\n",
    "- There different abbrevation for Street St, st, St.\n",
    "- Building and Bldg refrence Buildings\n",
    "\n",
    "In the next section we clean these qulaity issues and prpare the data for export into a database schema.\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning And Preparing\n",
    "\n",
    "In this section we will clean the data and replace the mismatches we discussed in the Data audit section.\n",
    "\n",
    "The code below will clean the data, and will use the function process_map to break the XML file into 5 different CSV files:\n",
    "- nodes.csv\n",
    "- nodes_tags.csv\n",
    "- ways.csv\n",
    "- ways_nodes.csv\n",
    "- ways_tags.csv\n",
    "\n",
    "Then read the data into these files based on the tag elements in the XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\",\"Way\", \"Circle\", \"Key\",\"Terrace\", \"Garden\"]\n",
    "\n",
    "# These are the variables I will fix and what they are being changed to\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "           \"Bldg\": \"Building\"\n",
    "            }\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "# Create Schema using the schema provided in the project instruction\n",
    "SCHEMA = schema = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "#               Helper Functions            \n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k : v for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    street=street_type_re.search(name).group()\n",
    "\n",
    "    name=name.replace(street, mapping[street])\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "#clean_element function take tag['value'] and tag['key'] as input and return the updated tag values \n",
    "def clean_element(tag_value, tag_key):\n",
    "    \n",
    "    ## clean postcode \n",
    "    if tag_key=='postcode':\n",
    "        if (tag_value[0:2]!='94' and tag_value[0:2]!='95') or (len(tag_value)!=5):\n",
    "            ## find postcode start with 'CA' and remove the 'CA' \n",
    "            if tag_value[0:2]=='CA': \n",
    "                    tag_value=tag_value[-5:]\n",
    "\n",
    "    ## clean street suffix, change abbrivations to full street suffix        \n",
    "    elif tag_key=='street':\n",
    "        street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "        full_addr=tag_value\n",
    "        m = street_type_re.search(full_addr)\n",
    "        if m:\n",
    "            street_type = m.group() #group(): Return the string matched by the RE\n",
    "            if street_type not in expected:\n",
    "                if street_type in mapping:\n",
    "                    tag_value=update_name(full_addr, mapping) # call update_name function \n",
    "    ## return updated tag_value\n",
    "    return tag_value\n",
    "                             \n",
    "## Clean and shape node or way XML element to Python dict\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=problemchars, default_tag_type='regular'):\n",
    "   \n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "   \n",
    "    ## clean node element\n",
    "    if element.tag=='node':\n",
    "        for primary in element.iter():\n",
    "            for i in node_attr_fields: \n",
    "                if i in primary.attrib: \n",
    "                    node_attribs[i]=primary.attrib[i]\n",
    "        if len(element)!=0:\n",
    "            for j in range(0, len(element)): \n",
    "                childelem=element[j]\n",
    "                tag={}\n",
    "                if not problem_chars.search(childelem.attrib['k']): ## ignor problematic element\n",
    "                    tag[\"id\"]=element.attrib[\"id\"]\n",
    "                    tag[\"type\"]=default_tag_type\n",
    "                    tag['value']=childelem.attrib['v']\n",
    "                    if \":\" in childelem.attrib['k']:\n",
    "                        k_and_v=childelem.attrib['k'].split(':',1)\n",
    "                        tag[\"type\"]=k_and_v[0]\n",
    "                        tag[\"key\"]=k_and_v[1]\n",
    "                        if tag[\"type\"]=='addr':\n",
    "                            tag[\"value\"]=clean_element(tag[\"value\"],tag[\"key\"]) ## call clean_element function\n",
    "                    else:\n",
    "                        tag[\"key\"]=childelem.attrib['k']\n",
    "                        if tag[\"type\"]=='addr':\n",
    "                            print(tag_value, tag[\"key\"])\n",
    "                            tag[\"value\"]=clean_element(tag[\"value\"],tag[\"key\"])\n",
    "                tags.append(tag)\n",
    "                \n",
    "        return ({'node': node_attribs, 'node_tags': tags})            \n",
    "                    \n",
    "    ## handle way element               \n",
    "    elif element.tag=='way':\n",
    "        for primary in element.iter():\n",
    "            for i in way_attr_fields: \n",
    "                if i in primary.attrib: \n",
    "                    way_attribs[i]=primary.attrib[i]   \n",
    "        \n",
    "        if len(element)!=0: \n",
    "            for j in range(0, len(element)): \n",
    "                childelem=element[j]\n",
    "                tag={}\n",
    "                if childelem.tag=='tag':\n",
    "                    if not problem_chars.search(childelem.attrib['k']):\n",
    "                        tag[\"id\"]=element.attrib[\"id\"]\n",
    "                        tag[\"type\"]=default_tag_type\n",
    "                        tag[\"value\"]=childelem.attrib['v']\n",
    "                        if \":\" in childelem.attrib['k']:\n",
    "                            k_and_v=childelem.attrib['k'].split(':',1)\n",
    "                            tag[\"key\"]=k_and_v[1]\n",
    "                            tag[\"type\"]=k_and_v[0]\n",
    "                            if tag[\"type\"]=='addr':\n",
    "                                tag[\"value\"]=clean_element(tag[\"value\"],tag[\"key\"]) #call clean_element function\n",
    "                        else:\n",
    "                            tag[\"key\"]=childelem.attrib['k']\n",
    "                            if tag[\"type\"]=='addr':\n",
    "                                tag[\"value\"]=clean_element(tag[\"value\"],tag[\"key\"]) #update tag values\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "                elif childelem.tag=='nd':\n",
    "                    #print (childelem.attrib['ref'])\n",
    "                    way_node={}\n",
    "                    way_node['id']=element.attrib['id'] \n",
    "                    way_node['node_id']=childelem.attrib['ref']\n",
    "                    way_node['position']=j\n",
    "                    #print(way_node)\n",
    "                    way_nodes.append(way_node)\n",
    "                    \n",
    "        return ({'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags})\n",
    "    \n",
    "## process the file, clean and write XML into csv according to given schema\n",
    "\n",
    "def process_map(file_in):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "    with codecs.open(NODES_PATH, 'w', encoding='utf-8') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w', encoding='utf-8') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w', encoding='utf-8') as ways_file, \\\n",
    "        codecs.open(WAY_NODES_PATH, 'w', encoding='utf-8') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w', encoding='utf-8') as way_tags_file:\n",
    "                \n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        \n",
    "        nodes_writer.writeheader()\n",
    "        \n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "    \n",
    "       \n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "                    \n",
    "process_map(\"SF_MAP.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sqllite Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref https://stackoverflow.com/questions/50735349/import-csv-into-sqlite3-insert-failed\n",
    "conn=sqlite3.connect('SF_MAP.db')\n",
    "cur = conn.cursor() \n",
    "cur.execute(\"CREATE TABLE nodes ( id INTEGER PRIMARY KEY NOT NULL, lat REAL, lon REAL,\\\n",
    "    user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT )\")\n",
    "conn.commit()\n",
    "node_df = pd.read_csv('nodes.csv', dtype=object)\n",
    "node_df.to_sql('nodes', conn, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "cur.execute(\"CREATE TABLE nodes_tags (\\\n",
    "    id INTEGER,\\\n",
    "    key TEXT,\\\n",
    "    value TEXT,\\\n",
    "    type TEXT,\\\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\\\n",
    ")\")\n",
    "conn.commit()\n",
    "nodetag_df=pd.read_csv('nodes_tags.csv')\n",
    "nodetag_df.to_sql('nodes_tags', conn, if_exists='append', index=False)\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways (\\\n",
    "    id INTEGER PRIMARY KEY NOT NULL,\\\n",
    "    user TEXT,\\\n",
    "    uid INTEGER,\\\n",
    "    version TEXT,\\\n",
    "    changeset INTEGER,\\\n",
    "    timestamp TEXT\\\n",
    ")\")\n",
    "conn.commit()\n",
    "way_df=pd.read_csv('ways.csv')\n",
    "way_df.to_sql('ways', conn, if_exists='append', index=False)\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_nodes (\\\n",
    "    id INTEGER NOT NULL,\\\n",
    "    node_id INTEGER NOT NULL, \\\n",
    "    position INTEGER NOT NULL, \\\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\\\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\\\n",
    ")\")\n",
    "conn.commit()\n",
    "waynode_df=pd.read_csv('ways_nodes.csv')\n",
    "waynode_df.to_sql('ways_nodes', conn, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_tags (\\\n",
    "    id INTEGER NOT NULL,\\\n",
    "    key TEXT NOT NULL,\\\n",
    "    value TEXT NOT NULL,\\\n",
    "    type TEXT,\\\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\\\n",
    ")\")\n",
    "conn.commit()\n",
    "waytag_df=pd.read_csv('ways_tags.csv')\n",
    "waytag_df=waytag_df.dropna(subset=['id', 'key', 'value'], how='any')\n",
    "waytag_df.to_sql('ways_tags', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Files Sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes.csv 19.54 MB\n",
      "nodes_tags.csv 1.82 MB\n",
      "schema.py 0.0 MB\n",
      "SF-MAP.ipynb 0.03 MB\n",
      "SF_MAP.db 28.39 MB\n",
      "SF_MAP.xml 57.9 MB\n",
      "ways.csv 1.63 MB\n",
      "ways_nodes.csv 7.09 MB\n",
      "ways_tags.csv 2.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Ref https://stackoverflow.com/questions/574730/python-how-to-ignore-an-exception-and-proceed\n",
    "# Ref https://stackabuse.com/python-list-files-in-a-directory/\n",
    "\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\".\"):  \n",
    "    try:\n",
    "        for filename in files:\n",
    "            print(filename, round(os.path.getsize(filename)/1000000, 2), 'MB')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Unique Users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(788,)\n"
     ]
    }
   ],
   "source": [
    "query='''select count(DISTINCT uid) from nodes; '''\n",
    "\n",
    "result=cur.execute(query)\n",
    "for row in result:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235055,)\n"
     ]
    }
   ],
   "source": [
    "query='''select count(DISTINCT id) from nodes; '''\n",
    "\n",
    "result=cur.execute(query)\n",
    "for row in result:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Ways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26374,)\n"
     ]
    }
   ],
   "source": [
    "query='''select count(DISTINCT id) from ways; '''\n",
    "\n",
    "result=cur.execute(query)\n",
    "for row in result:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Bars in the area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "query='''select count(value) from nodes_tags where key = 'amenity' AND (value = 'bar' or value = 'pub'); '''\n",
    "\n",
    "result=cur.execute(query)\n",
    "for row in result:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Place of Worship in the area**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52,)\n"
     ]
    }
   ],
   "source": [
    "query='''select count(value) from nodes_tags where key = 'amenity' AND value LIKE '%worship%'; '''\n",
    "\n",
    "result=cur.execute(query)\n",
    "for row in result:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Schools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "query='''select count(value) from nodes_tags where key = 'amenity' AND value LIKE '%School%'; '''\n",
    "\n",
    "result=cur.execute(query)\n",
    "for row in result:\n",
    "    print (row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions And Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although OpenStreetMaps has guidance for the contributors to follow I suggest they have data validation rules when the data is being entered. The data validation will help with keeping the data consistent and will minimize errors. Also, there are many opportunities to bucket places into one category, for example, pubs and bars are separated in the amenity category but might be better to combine them into one, cinemas and theaters can also be combined.\n",
    "\n",
    "This can help OpenStreetMaps to better manage the data, which will also take up less space and will give the users less options to choose from when tagging locations\n",
    "\n",
    "Below query shows all ameneties and how we can bucket some of the sub categories into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pub',)\n",
      "('bar',)\n",
      "('post_office',)\n",
      "('parking_entrance',)\n",
      "('cinema',)\n",
      "('fuel',)\n",
      "('ferry_terminal',)\n",
      "('parking',)\n",
      "('post_box',)\n",
      "('cafe',)\n",
      "('nightclub',)\n",
      "('drinking_water',)\n",
      "('toilets',)\n",
      "('bicycle_parking',)\n",
      "('restaurant',)\n",
      "('telephone',)\n",
      "('library',)\n",
      "('fast_food',)\n",
      "('bank',)\n",
      "('school',)\n",
      "('place_of_worship',)\n",
      "('university',)\n",
      "('fountain',)\n",
      "('language_school',)\n",
      "('college',)\n",
      "('nursing_home',)\n",
      "('theatre',)\n",
      "('ice_cream',)\n",
      "('arts_centre',)\n",
      "('fire_station',)\n",
      "('public_building',)\n",
      "('police',)\n",
      "('atm',)\n",
      "('taxi',)\n",
      "('car_rental',)\n",
      "('brokerage',)\n",
      "('car_sharing',)\n",
      "('clock',)\n",
      "('gym',)\n",
      "('community_centre',)\n",
      "('pharmacy',)\n",
      "('dentist',)\n",
      "('salon',)\n",
      "('hookah_lounge',)\n",
      "('club',)\n",
      "('bench',)\n",
      "('vending_machine',)\n",
      "('event',)\n",
      "('social_centre',)\n",
      "('social_facility',)\n",
      "('kindergarten',)\n",
      "('carpool',)\n",
      "('clinic',)\n",
      "('waste_basket',)\n",
      "('stripclub',)\n",
      "('footpath',)\n",
      "('doctors',)\n",
      "('marketplace',)\n",
      "('shelter',)\n",
      "('recycling',)\n",
      "('embassy',)\n",
      "('coworking_space',)\n",
      "('conference_centre',)\n",
      "('bicycle_rental',)\n",
      "('vintage and modern resale',)\n",
      "('dormitory',)\n",
      "('bureau_de_change',)\n",
      "('studio',)\n",
      "('veterinary',)\n",
      "('ticket_validator',)\n",
      "('ticket_booth',)\n",
      "('food_court',)\n",
      "('storage',)\n",
      "('loading_dock',)\n",
      "('table',)\n",
      "('photo_booth',)\n"
     ]
    }
   ],
   "source": [
    "query='''select DISTINCT value from nodes_tags where key = 'amenity'; '''\n",
    "\n",
    "result=cur.execute(query)\n",
    "for row in result:\n",
    "    print (row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
